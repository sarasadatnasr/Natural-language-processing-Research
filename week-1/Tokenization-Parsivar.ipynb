{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsivar-Tokenizer**\n",
    "\n",
    "https://pypi.org/project/parsivar/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parsivar in c:\\users\\user\\anaconda3\\lib\\site-packages (0.2.3)\n",
      "Collecting nltk==3.4.5\n",
      "  Using cached nltk-3.4.5-py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk==3.4.5->parsivar) (1.12.0)\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.3\n",
      "    Uninstalling nltk-3.3:\n",
      "      Successfully uninstalled nltk-3.3\n",
      "Successfully installed nltk-3.4.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "parsinorm 0.0.3 requires nltk==3.3, but you have nltk 3.4.5 which is incompatible.\n",
      "hazm 0.7.0 requires nltk==3.3, but you have nltk 3.4.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install parsivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsivar import Tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نشانۀ جمع «ها»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['متن',\n",
       " 'گفتمانی',\n",
       " 'است',\n",
       " 'بیان\\u200cکننده',\n",
       " 'احساسات،',\n",
       " 'عواطف،',\n",
       " 'کنش\\u200cها،',\n",
       " 'واکنش\\u200cها.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('متن گفتمانی است بیان‌کننده احساسات، عواطف، کنش‌ها، واکنش‌ها.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ها» شبه جمله"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['چه\\u200cکار', 'می\\u200cکنی؟', 'ها؟', 'می\\u200cزنیم؟', 'ها؟']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('چه‌کار می‌کنی؟ ها؟ می‌زنیم؟‌ ها؟')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "واژه‌های مرکب جدا"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['با',\n",
       " 'حشره\\u200cکش،',\n",
       " 'پروانه\\u200cها',\n",
       " 'را',\n",
       " 'دیوانه\\u200cوار',\n",
       " 'می\\u200cکُشد',\n",
       " 'و',\n",
       " 'می\\u200cگوید...',\n",
       " 'پروانه\\u200cها',\n",
       " 'بدیمن\\u200cاند.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('با حشره‌کش، پروانه‌ها را دیوانه‌وار می‌کُشد و می‌گوید... پروانه‌ها بدیمن‌اند.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل ‌هایی با دو بخش جدا«باشم شدم ...»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ناگهان', 'فهمید', 'کور', 'خوانده\\u200cاست.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('ناگهان فهمید کور خوانده‌است.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل ‌هایی با دو بخش چسبیده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['من،',\n",
       " 'همه',\n",
       " 'عمر،',\n",
       " 'آن\\u200cچه',\n",
       " 'گفته\\u200cام',\n",
       " 'از',\n",
       " 'سیاست',\n",
       " 'گفته\\u200cام.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('من، همه عمر، آن‌چه گفته‌ام از سیاست گفته‌ام.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل مرکب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['و', 'هرگز', 'دست', 'از', 'پا', 'خطا', 'نمی\\u200cکند']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('و هرگز دست از پا خطا نمی‌کند')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل شبه مرکب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['با', 'حرف', 'های', 'خود', 'کارها', 'را', 'خراب', 'کرد.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('با حرف های خود کارها را خراب کرد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "افعال دارای «می»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['من',\n",
       " 'نیز',\n",
       " 'می\\u200cخواسته\\u200cام',\n",
       " 'که',\n",
       " 'مرا',\n",
       " 'به\\u200cزنهار',\n",
       " 'خود',\n",
       " 'درآرد.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('من نیز می‌خواسته‌ام که مرا به‌زنهار خود درآرد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ی»اضافه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ز',\n",
       " 'معمای',\n",
       " 'سربستۀ',\n",
       " 'مشکل\\u200cگشای',\n",
       " 'هزار',\n",
       " 'اسم',\n",
       " 'گوناگون',\n",
       " 'بیرون',\n",
       " 'می\\u200cآورد']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('ز معمای سربستۀ مشکل‌گشای هزار اسم گوناگون بیرون می‌آورد')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ای» نکره"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['از',\n",
       " 'خانه\\u200cای',\n",
       " 'به\\u200cدر',\n",
       " 'می\\u200cآمدم',\n",
       " 'و',\n",
       " 'در',\n",
       " 'خانه\\u200cای',\n",
       " 'دیگر',\n",
       " 'می\\u200cرفتم.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('از خانه‌ای به‌در می‌آمدم و در خانه‌ای دیگر می‌رفتم.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن برخی پسوندها مانند «تر» و «ترین»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['شیطان\\u200cترین',\n",
       " 'بچّه\\u200cهای',\n",
       " 'شیراز',\n",
       " 'بچّه\\u200cهای',\n",
       " 'دروازه',\n",
       " 'سعدی',\n",
       " 'هستند.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('شیطان‌ترین بچّه‌های شیراز بچّه‌های دروازه سعدی هستند.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "واژه «تر»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['زمانی\\u200cکه',\n",
       " 'از',\n",
       " 'لُنگِ',\n",
       " 'تر،',\n",
       " 'خط\\u200cهای',\n",
       " 'قرمزی',\n",
       " 'بر',\n",
       " 'بدن',\n",
       " 'ما',\n",
       " 'نقش',\n",
       " 'بست،',\n",
       " 'بابام',\n",
       " 'آرام',\n",
       " 'شد.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('زمانی‌که از لُنگِ تر، خط‌های قرمزی بر بدن ما نقش بست، بابام آرام شد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن برخی پسوندها مانند «شده» "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['سمرقند',\n",
       " 'زاده',\n",
       " 'و',\n",
       " 'به',\n",
       " 'باورد',\n",
       " 'بزرگ\\u200cشده',\n",
       " 'و',\n",
       " 'کوفی',\n",
       " 'اصل',\n",
       " 'است.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('سمرقند زاده و به باورد بزرگ‌شده و کوفی اصل است.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "قواعد جدانویسی : برای مثال، اگر دو کلمۀ «آن» و «این» قبل از یک اسم بیاین"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['شوهر',\n",
       " 'از',\n",
       " 'مشاهدت',\n",
       " 'آن',\n",
       " 'حال',\n",
       " 'بر',\n",
       " 'جفای',\n",
       " 'زن',\n",
       " 'پشیمانی',\n",
       " 'تمام',\n",
       " 'خورد.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('شوهر از مشاهدت آن‌ حال بر جفای زن پشیمانی تمام خورد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کننده غبر قطعی مثل ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['در',\n",
       " 'اوایل',\n",
       " 'رمضان',\n",
       " 'سال',\n",
       " '1291',\n",
       " 'ه.ق',\n",
       " 'در',\n",
       " 'تبریز',\n",
       " 'به\\u200cدنیا',\n",
       " 'آمد.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('در اوایل رمضان سال 1291 ه.ق در تبریز به‌دنیا آمد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا نکردن پشوند و پسوند"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['حتی', 'پرسیدم', 'که', 'بهشان', 'برخورد', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('حتی پرسیدم که بهشان برخورد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن حرف ندا "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['خدایا', '،', 'خدایا', '،', 'چرا', 'مرا', 'واگذاشتی', '؟']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('خدایا، خدایا، چرا مرا واگذاشتی؟')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن ست مانند : اوست"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['هم', 'اوست', 'عاشق', '،', 'هم', 'اوست', 'معشوق', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_words('هم اوست عاشق، هم اوست معشوق.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
