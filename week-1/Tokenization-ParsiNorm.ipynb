{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ParsiNorm-Tokenizer**\n",
    "\n",
    "https://github.com/haraai/ParsiNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parsinorm in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "parsivar 0.2.3 requires nltk==3.4.5, but you have nltk 3.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: persian-tools==0.0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from parsinorm) (0.0.10)\n",
      "Requirement already satisfied: urlextract==1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from parsinorm) (1.4.0)\n",
      "Collecting nltk==3.3\n",
      "  Using cached nltk-3.3-py3-none-any.whl\n",
      "Requirement already satisfied: hazm==0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from parsinorm) (0.7.0)\n",
      "Requirement already satisfied: num2fawords==1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from parsinorm) (1.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk==3.3->parsinorm) (1.12.0)\n",
      "Requirement already satisfied: uritools in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from urlextract==1.4.0->parsinorm) (4.0.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from urlextract==1.4.0->parsinorm) (1.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from urlextract==1.4.0->parsinorm) (3.7.1)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from urlextract==1.4.0->parsinorm) (3.3)\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.4.5\n",
      "    Uninstalling nltk-3.4.5:\n",
      "      Successfully uninstalled nltk-3.4.5\n",
      "Successfully installed nltk-3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install parsinorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsinorm import Tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نشانۀ جمع «ها»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['متن',\n",
       " 'گفتمانی',\n",
       " 'است',\n",
       " 'بیان\\u200cکننده',\n",
       " 'احساسات',\n",
       " '،',\n",
       " 'عواطف',\n",
       " '،',\n",
       " 'کنش\\u200cها',\n",
       " '،',\n",
       " 'واکنش\\u200cها',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('متن گفتمانی است بیان‌کننده احساسات، عواطف، کنش‌ها، واکنش‌ها.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ها» شبه جمله"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['چه\\u200cکار', 'می\\u200cکنی', '؟', 'ها', '؟', 'می\\u200cزنیم', '؟', 'ها', '؟']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('چه‌کار می‌کنی؟ ها؟ می‌زنیم؟‌ ها؟')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "واژه‌های مرکب جدا"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['با',\n",
       " 'حشره\\u200cکش',\n",
       " '،',\n",
       " 'پروانه\\u200cها',\n",
       " 'را',\n",
       " 'دیوانه\\u200cوار',\n",
       " 'می\\u200cکُشد',\n",
       " 'و',\n",
       " 'می\\u200cگوید',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'پروانه\\u200cها',\n",
       " 'بدیمن\\u200cاند',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('با حشره‌کش، پروانه‌ها را دیوانه‌وار می‌کُشد و می‌گوید... پروانه‌ها بدیمن‌اند.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل ‌هایی با دو بخش جدا«باشم شدم ...»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ناگهان', 'فهمید', 'کور', 'خوانده', 'است', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('ناگهان فهمید کور خوانده‌ است.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل ‌هایی با دو بخش چسبیده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['من',\n",
       " '،',\n",
       " 'همه',\n",
       " 'عمر',\n",
       " '،',\n",
       " 'آن\\u200cچه',\n",
       " 'گفته\\u200cام',\n",
       " 'از',\n",
       " 'سیاست',\n",
       " 'گفته\\u200cام',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('من، همه عمر، آن‌چه گفته‌ام از سیاست گفته‌ام.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل مرکب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['و', 'هرگز', 'دست', 'از', 'پا', 'خطا', 'نمی\\u200cکند']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('و هرگز دست از پا خطا نمی‌کند')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل شبه مرکب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['با', 'حرف', 'های', 'خود', 'کارها', 'را', 'خراب', 'کرد', '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('با حرف های خود کارها را خراب کرد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "افعال دارای «می»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['من',\n",
       " 'نیز',\n",
       " 'می\\u200cخواسته\\u200cام',\n",
       " 'که',\n",
       " 'مرا',\n",
       " 'به\\u200cزنهار',\n",
       " 'خود',\n",
       " 'درآرد.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('من نیز می‌خواسته‌ام که مرا به‌زنهار خود درآرد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ی»اضافه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ز',\n",
       " 'معمای',\n",
       " 'سربستۀ',\n",
       " 'مشکل\\u200cگشای',\n",
       " 'هزار',\n",
       " 'اسم',\n",
       " 'گوناگون',\n",
       " 'بیرون',\n",
       " 'می\\u200cآورد']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('ز معمای سربستۀ مشکل‌گشای هزار اسم گوناگون بیرون می‌آورد')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ای» نکره"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['از',\n",
       " 'خانه\\u200cای',\n",
       " 'به\\u200cدر',\n",
       " 'می\\u200cآمدم',\n",
       " 'و',\n",
       " 'در',\n",
       " 'خانه\\u200cای',\n",
       " 'دیگر',\n",
       " 'می\\u200cرفتم',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('از خانه‌ای به‌در می‌آمدم و در خانه‌ای دیگر می‌رفتم.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن برخی پسوندها مانند «تر» و «ترین»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['شیطان\\u200cترین',\n",
       " 'بچّه\\u200cهای',\n",
       " 'شیراز',\n",
       " 'بچّه\\u200cهای',\n",
       " 'دروازه',\n",
       " 'سعدی',\n",
       " 'هستند',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('شیطان‌ترین بچّه‌های شیراز بچّه‌های دروازه سعدی هستند.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "واژه «تر»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['زمانی\\u200cکه',\n",
       " 'از',\n",
       " 'لُنگِ',\n",
       " 'تر',\n",
       " '،',\n",
       " 'خط\\u200cهای',\n",
       " 'قرمزی',\n",
       " 'بر',\n",
       " 'بدن',\n",
       " 'ما',\n",
       " 'نقش',\n",
       " 'بست',\n",
       " '،',\n",
       " 'بابام',\n",
       " 'آرام',\n",
       " 'شد',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('زمانی‌که از لُنگِ تر، خط‌های قرمزی بر بدن ما نقش بست، بابام آرام شد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن برخی پسوندها مانند «شده» "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['سمرقند',\n",
       " 'زاده',\n",
       " 'و',\n",
       " 'به',\n",
       " 'باورد',\n",
       " 'بزرگ\\u200cشده',\n",
       " 'و',\n",
       " 'کوفی',\n",
       " 'اصل',\n",
       " 'است',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('سمرقند زاده و به باورد بزرگ‌شده و کوفی اصل است.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "قواعد جدانویسی : برای مثال، اگر دو کلمۀ «آن» و «این» قبل از یک اسم بیاین"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['شوهر',\n",
       " 'از',\n",
       " 'مشاهدت',\n",
       " 'آن',\n",
       " 'حال',\n",
       " 'بر',\n",
       " 'جفای',\n",
       " 'زن',\n",
       " 'پشیمانی',\n",
       " 'تمام',\n",
       " 'خورد',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('شوهر از مشاهدت آن‌ حال بر جفای زن پشیمانی تمام خورد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کننده غبر قطعی مثل ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['در',\n",
       " 'اوایل',\n",
       " 'رمضان',\n",
       " 'سال',\n",
       " '1291',\n",
       " 'ه.ق',\n",
       " 'در',\n",
       " 'تبریز',\n",
       " 'به\\u200cدنیا',\n",
       " 'آمد.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('در اوایل رمضان سال 1291 ه.ق در تبریز به‌دنیا آمد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا نکردن پشوند و پسوند"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['حتی', 'پرسیدم', 'که', 'بهشان', 'برخورد', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('حتی پرسیدم که بهشان برخورد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن حرف ندا "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['خدایا', '،', 'خدایا', '،', 'چرا', 'مرا', 'واگذاشتی', '؟']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('خدایا، خدایا، چرا مرا واگذاشتی؟')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن ست مانند : اوست"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['هم', 'اوست', 'عاشق', '،', 'هم', 'اوست', 'معشوق', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenize('هم اوست عاشق، هم اوست معشوق.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
