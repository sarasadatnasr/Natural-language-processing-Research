{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HAZM**\n",
    "\n",
    "https://www.roshan-ai.ir/hazm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk==3.3->hazm) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Sequence' from 'collections' (c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\collections\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\NLP_research\\Tokenization-Hazm.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/NLP_research/Tokenization-Hazm.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m unicode_literals\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/NLP_research/Tokenization-Hazm.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhazm\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\hazm\\__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mWordTokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m WordTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mSentenceTokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTokenizer\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mTokenSplitter\u001b[39;00m \u001b[39mimport\u001b[39;00m TokenSplitter\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\hazm\\WordTokenizer.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mcodecs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m words_list, default_words, default_verbs\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m TokenizerI\n\u001b[0;32m      9\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mWordTokenizer\u001b[39;00m(TokenizerI):\n\u001b[0;32m     10\u001b[0m \t\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m\t>>> tokenizer = WordTokenizer()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m\t>>> tokenizer.tokenize('این جمله (خیلی) پیچیده نیست!!!')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m\tدیگه میخوام ترک تحصیل کنم 😂 😂 😂\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39m\t\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\__init__.py:134\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrammar\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    133\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m--> 134\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    135\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    136\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\text.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msix\u001b[39;00m \u001b[39mimport\u001b[39;00m text_type\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlm\u001b[39;00m \u001b[39mimport\u001b[39;00m MLE\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m padded_everygram_pipeline\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m FreqDist\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\lm\\__init__.py:222\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Natural Language Toolkit: Language Models\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m# URL: <http://nltk.org/\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mNLTK Language Modeling Module.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m------------------------------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39mwill be ignored.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    223\u001b[0m     MLE,\n\u001b[0;32m    224\u001b[0m     Lidstone,\n\u001b[0;32m    225\u001b[0m     Laplace,\n\u001b[0;32m    226\u001b[0m     WittenBellInterpolated,\n\u001b[0;32m    227\u001b[0m     KneserNeyInterpolated,\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcounter\u001b[39;00m \u001b[39mimport\u001b[39;00m NgramCounter\n\u001b[0;32m    230\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvocabulary\u001b[39;00m \u001b[39mimport\u001b[39;00m Vocabulary\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\lm\\models.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m division, unicode_literals\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m LanguageModel, Smoothing\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msmoothing\u001b[39;00m \u001b[39mimport\u001b[39;00m KneserNey, WittenBell\n\u001b[0;32m     16\u001b[0m \u001b[39m@compat\u001b[39m\u001b[39m.\u001b[39mpython_2_unicode_compatible\n\u001b[0;32m     17\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMLE\u001b[39;00m(LanguageModel):\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\lm\\api.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbisect\u001b[39;00m \u001b[39mimport\u001b[39;00m bisect\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msix\u001b[39;00m \u001b[39mimport\u001b[39;00m add_metaclass\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcounter\u001b[39;00m \u001b[39mimport\u001b[39;00m NgramCounter\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m log_base2\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvocabulary\u001b[39;00m \u001b[39mimport\u001b[39;00m Vocabulary\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\lm\\counter.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mLanguage Model Counter\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m----------------------\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m unicode_literals\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequence, defaultdict\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msix\u001b[39;00m \u001b[39mimport\u001b[39;00m string_types\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Sequence' from 'collections' (c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\collections\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نشانۀ جمع «ها»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['متن',\n",
       " 'گفتمانی',\n",
       " 'است',\n",
       " 'بیان\\u200cکننده',\n",
       " 'احساسات',\n",
       " '،',\n",
       " 'عواطف',\n",
       " '،',\n",
       " 'کنش\\u200cها',\n",
       " '،',\n",
       " 'واکنش\\u200cها',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('متن گفتمانی است بیان‌کننده احساسات، عواطف، کنش‌ها، واکنش‌ها.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ها» شبه جمله"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\NLP_research\\Tokenization-Hazm.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/NLP_research/Tokenization-Hazm.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m word_tokenize (\u001b[39m'\u001b[39m\u001b[39mچه‌کار می‌کنی؟ ها؟ می‌زنیم؟‌ ها؟\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "word_tokenize ('چه‌کار می‌کنی؟ ها؟ می‌زنیم؟‌ ها؟')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "واژه‌های مرکب جدا"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['با',\n",
       " 'حشره\\u200cکش',\n",
       " '،',\n",
       " 'پروانه\\u200cها',\n",
       " 'را',\n",
       " 'دیوانه\\u200cوار',\n",
       " 'می\\u200cکُشد',\n",
       " 'و',\n",
       " 'می\\u200cگوید',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'پروانه\\u200cها',\n",
       " 'بدیمن\\u200cاند',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('با حشره‌کش، پروانه‌ها را دیوانه‌وار می‌کُشد و می‌گوید... پروانه‌ها بدیمن‌اند.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل ‌هایی با دو بخش جدا«باشم شدم ...»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ناگهان', 'فهمید', 'کور', 'خوانده\\u200cاست', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('ناگهان فهمید کور خوانده‌است.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل ‌هایی با دو بخش چسبیده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['من',\n",
       " '،',\n",
       " 'همه',\n",
       " 'عمر',\n",
       " '،',\n",
       " 'آن\\u200cچه',\n",
       " 'گفته\\u200cام',\n",
       " 'از',\n",
       " 'سیاست',\n",
       " 'گفته\\u200cام',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('من، همه عمر، آن‌چه گفته‌ام از سیاست گفته‌ام.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل مرکب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['و', 'هرگز', 'دست', 'از', 'پا', 'خطا', 'نمی', 'کند']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('و هرگز دست از پا خطا نمی کند')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل شبه مرکب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize ('با حرف های خود کارها را خراب کرد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "افعال دارای «می»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['من',\n",
       " 'نیز',\n",
       " 'می\\u200cخواسته\\u200cام',\n",
       " 'که',\n",
       " 'مرا',\n",
       " 'به\\u200cزنهار',\n",
       " 'خود',\n",
       " 'درآرد',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('من نیز می‌خواسته‌ام که مرا به‌زنهار خود درآرد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ی»اضافه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ز',\n",
       " 'معمای',\n",
       " 'سربستۀ',\n",
       " 'مشکل\\u200cگشای',\n",
       " 'هزار',\n",
       " 'اسم',\n",
       " 'گوناگون',\n",
       " 'بیرون',\n",
       " 'می\\u200cآورد']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('ز معمای سربستۀ مشکل‌گشای هزار اسم گوناگون بیرون می‌آورد')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ای» نکره"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['از',\n",
       " 'خانه\\u200cای',\n",
       " 'به\\u200cدر',\n",
       " 'می\\u200cآمدم',\n",
       " 'و',\n",
       " 'در',\n",
       " 'خانه\\u200cای',\n",
       " 'دیگر',\n",
       " 'می\\u200cرفتم',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('از خانه‌ای به‌در می‌آمدم و در خانه‌ای دیگر می‌رفتم.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن برخی پسوندها مانند «تر» و «ترین»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['شیطان\\u200cترین',\n",
       " 'بچّه\\u200cهای',\n",
       " 'شیراز',\n",
       " 'بچّه\\u200cهای',\n",
       " 'دروازه',\n",
       " 'سعدی',\n",
       " 'هستند',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('شیطان‌ترین بچّه‌های شیراز بچّه‌های دروازه سعدی هستند.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "واژه «تر»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['زمانی\\u200cکه',\n",
       " 'از',\n",
       " 'لُنگِ',\n",
       " 'تر',\n",
       " '،',\n",
       " 'خط\\u200cهای',\n",
       " 'قرمزی',\n",
       " 'بر',\n",
       " 'بدن',\n",
       " 'ما',\n",
       " 'نقش',\n",
       " 'بست',\n",
       " '،',\n",
       " 'بابام',\n",
       " 'آرام',\n",
       " 'شد',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('زمانی‌که از لُنگِ تر، خط‌های قرمزی بر بدن ما نقش بست، بابام آرام شد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن برخی پسوندها مانند «شده» "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['سمرقند',\n",
       " 'زاده',\n",
       " 'و',\n",
       " 'به',\n",
       " 'باورد',\n",
       " 'بزرگ\\u200cشده',\n",
       " 'و',\n",
       " 'کوفی',\n",
       " 'اصل',\n",
       " 'است',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('سمرقند زاده و به باورد بزرگ‌شده و کوفی اصل است.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "قواعد جدانویسی : برای مثال، اگر دو کلمۀ «آن» و «این» قبل از یک اسم بیاین"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['شوهر',\n",
       " 'از',\n",
       " 'مشاهدت',\n",
       " 'آن\\u200c',\n",
       " 'حال',\n",
       " 'بر',\n",
       " 'جفای',\n",
       " 'زن',\n",
       " 'پشیمانی',\n",
       " 'تمام',\n",
       " 'خورد',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('شوهر از مشاهدت آن‌ حال بر جفای زن پشیمانی تمام خورد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کننده غبر قطعی مثل ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['در',\n",
       " 'اوایل',\n",
       " 'رمضان',\n",
       " 'سال',\n",
       " '1291',\n",
       " 'ه',\n",
       " '.',\n",
       " 'ق',\n",
       " 'در',\n",
       " 'تبریز',\n",
       " 'به\\u200cدنیا',\n",
       " 'آمد',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('در اوایل رمضان سال 1291 ه.ق در تبریز به‌دنیا آمد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا نکردن پشوند و پسوند"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['حتی', 'پرسیدم', 'که', 'بهشان', 'برخورد', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('حتی پرسیدم که بهشان برخورد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن حرف ندا "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['خدایا', '،', 'خدایا', '،', 'چرا', 'مرا', 'واگذاشتی', '؟']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('خدایا، خدایا، چرا مرا واگذاشتی؟')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن ست مانند : اوست"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['هم', 'اوست', 'عاشق', '،', 'هم', 'اوست', 'معشوق', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('هم اوست عاشق، هم اوست معشوق.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
