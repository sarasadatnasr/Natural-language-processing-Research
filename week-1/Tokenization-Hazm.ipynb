{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HAZM**\n",
    "\n",
    "https://www.roshan-ai.ir/hazm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting hazm\n",
      "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 KB\u001b[0m \u001b[31m707.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting libwapiti>=0.2.1\n",
      "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 KB\u001b[0m \u001b[31m489.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk==3.3\n",
      "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from nltk==3.3->hazm) (1.16.0)\n",
      "Building wheels for collected packages: nltk, libwapiti\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394485 sha256=6a9ba263d05605f58b445d599d86dbd3daa3d07c4d25217f553266afca218e8a\n",
      "  Stored in directory: /home/sars/.cache/pip/wheels/6b/6d/14/3defa4cd7013faeddf715150696f4a96d7725c87700eb8a68e\n",
      "  Building wheel for libwapiti (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp310-cp310-linux_x86_64.whl size=179093 sha256=b58d36cf3175a4f2b1cfd3b7487177fb2aad911641acf3cb713dc206ed669551\n",
      "  Stored in directory: /home/sars/.cache/pip/wheels/9f/cb/30/fef48ecac051e433987eccdb5682900b4c00d44a4bcd4d4ec8\n",
      "Successfully built nltk libwapiti\n",
      "Installing collected packages: nltk, libwapiti, hazm\n",
      "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نشانۀ جمع «ها»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['متن',\n",
       " 'گفتمانی',\n",
       " 'است',\n",
       " 'بیان\\u200cکننده',\n",
       " 'احساسات',\n",
       " '،',\n",
       " 'عواطف',\n",
       " '،',\n",
       " 'کنش\\u200cها',\n",
       " '،',\n",
       " 'واکنش\\u200cها',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('متن گفتمانی است بیان‌کننده احساسات، عواطف، کنش‌ها، واکنش‌ها.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ها» شبه جمله"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\NLP_research\\Tokenization-Hazm.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/NLP_research/Tokenization-Hazm.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m word_tokenize (\u001b[39m'\u001b[39m\u001b[39mچه‌کار می‌کنی؟ ها؟ می‌زنیم؟‌ ها؟\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "word_tokenize ('چه‌کار می‌کنی؟ ها؟ می‌زنیم؟‌ ها؟')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "واژه‌های مرکب جدا"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['با',\n",
       " 'حشره\\u200cکش',\n",
       " '،',\n",
       " 'پروانه\\u200cها',\n",
       " 'را',\n",
       " 'دیوانه\\u200cوار',\n",
       " 'می\\u200cکُشد',\n",
       " 'و',\n",
       " 'می\\u200cگوید',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'پروانه\\u200cها',\n",
       " 'بدیمن\\u200cاند',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('با حشره‌کش، پروانه‌ها را دیوانه‌وار می‌کُشد و می‌گوید... پروانه‌ها بدیمن‌اند.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل ‌هایی با دو بخش جدا«باشم شدم ...»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ناگهان', 'فهمید', 'کور', 'خوانده\\u200cاست', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('ناگهان فهمید کور خوانده‌است.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل ‌هایی با دو بخش چسبیده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['من',\n",
       " '،',\n",
       " 'همه',\n",
       " 'عمر',\n",
       " '،',\n",
       " 'آن\\u200cچه',\n",
       " 'گفته\\u200cام',\n",
       " 'از',\n",
       " 'سیاست',\n",
       " 'گفته\\u200cام',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('من، همه عمر، آن‌چه گفته‌ام از سیاست گفته‌ام.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل مرکب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['و', 'هرگز', 'دست', 'از', 'پا', 'خطا', 'نمی', 'کند']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('و هرگز دست از پا خطا نمی کند')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فعل شبه مرکب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize ('با حرف های خود کارها را خراب کرد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "افعال دارای «می»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['من',\n",
       " 'نیز',\n",
       " 'می\\u200cخواسته\\u200cام',\n",
       " 'که',\n",
       " 'مرا',\n",
       " 'به\\u200cزنهار',\n",
       " 'خود',\n",
       " 'درآرد',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('من نیز می‌خواسته‌ام که مرا به‌زنهار خود درآرد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ی»اضافه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ز',\n",
       " 'معمای',\n",
       " 'سربستۀ',\n",
       " 'مشکل\\u200cگشای',\n",
       " 'هزار',\n",
       " 'اسم',\n",
       " 'گوناگون',\n",
       " 'بیرون',\n",
       " 'می\\u200cآورد']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('ز معمای سربستۀ مشکل‌گشای هزار اسم گوناگون بیرون می‌آورد')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«ای» نکره"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['از',\n",
       " 'خانه\\u200cای',\n",
       " 'به\\u200cدر',\n",
       " 'می\\u200cآمدم',\n",
       " 'و',\n",
       " 'در',\n",
       " 'خانه\\u200cای',\n",
       " 'دیگر',\n",
       " 'می\\u200cرفتم',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('از خانه‌ای به‌در می‌آمدم و در خانه‌ای دیگر می‌رفتم.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن برخی پسوندها مانند «تر» و «ترین»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['شیطان\\u200cترین',\n",
       " 'بچّه\\u200cهای',\n",
       " 'شیراز',\n",
       " 'بچّه\\u200cهای',\n",
       " 'دروازه',\n",
       " 'سعدی',\n",
       " 'هستند',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('شیطان‌ترین بچّه‌های شیراز بچّه‌های دروازه سعدی هستند.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "واژه «تر»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['زمانی\\u200cکه',\n",
       " 'از',\n",
       " 'لُنگِ',\n",
       " 'تر',\n",
       " '،',\n",
       " 'خط\\u200cهای',\n",
       " 'قرمزی',\n",
       " 'بر',\n",
       " 'بدن',\n",
       " 'ما',\n",
       " 'نقش',\n",
       " 'بست',\n",
       " '،',\n",
       " 'بابام',\n",
       " 'آرام',\n",
       " 'شد',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('زمانی‌که از لُنگِ تر، خط‌های قرمزی بر بدن ما نقش بست، بابام آرام شد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن برخی پسوندها مانند «شده» "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['سمرقند',\n",
       " 'زاده',\n",
       " 'و',\n",
       " 'به',\n",
       " 'باورد',\n",
       " 'بزرگ\\u200cشده',\n",
       " 'و',\n",
       " 'کوفی',\n",
       " 'اصل',\n",
       " 'است',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('سمرقند زاده و به باورد بزرگ‌شده و کوفی اصل است.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "قواعد جدانویسی : برای مثال، اگر دو کلمۀ «آن» و «این» قبل از یک اسم بیاین"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['شوهر',\n",
       " 'از',\n",
       " 'مشاهدت',\n",
       " 'آن\\u200c',\n",
       " 'حال',\n",
       " 'بر',\n",
       " 'جفای',\n",
       " 'زن',\n",
       " 'پشیمانی',\n",
       " 'تمام',\n",
       " 'خورد',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('شوهر از مشاهدت آن‌ حال بر جفای زن پشیمانی تمام خورد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کننده غبر قطعی مثل ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['در',\n",
       " 'اوایل',\n",
       " 'رمضان',\n",
       " 'سال',\n",
       " '1291',\n",
       " 'ه',\n",
       " '.',\n",
       " 'ق',\n",
       " 'در',\n",
       " 'تبریز',\n",
       " 'به\\u200cدنیا',\n",
       " 'آمد',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('در اوایل رمضان سال 1291 ه.ق در تبریز به‌دنیا آمد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا نکردن پشوند و پسوند"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['حتی', 'پرسیدم', 'که', 'بهشان', 'برخورد', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('حتی پرسیدم که بهشان برخورد.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن حرف ندا "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['خدایا', '،', 'خدایا', '،', 'چرا', 'مرا', 'واگذاشتی', '؟']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('خدایا، خدایا، چرا مرا واگذاشتی؟')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جدا کردن ست مانند : اوست"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['هم', 'اوست', 'عاشق', '،', 'هم', 'اوست', 'معشوق', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize ('هم اوست عاشق، هم اوست معشوق.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
